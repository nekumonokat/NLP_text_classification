{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d423c27a",
   "metadata": {},
   "source": [
    "<h2><center>NLP Text Classification</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d0067",
   "metadata": {},
   "source": [
    "## I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101e842",
   "metadata": {},
   "source": [
    "### 1.1 Domain-specific area\n",
    "This project provides an analysis of textual data on Twitter to accurately detect and classify sentiment trends regarding elections in India using sentiment analysis techniques. This would provide government bodies and political parties with a tool that takes in a corpus of text for training to analyse the sentiment of tweets and assist in decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72286743",
   "metadata": {},
   "source": [
    "### 1.2 Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd0cf1",
   "metadata": {},
   "source": [
    "### 1.3 Dataset\n",
    "To begin this project, an extensive amount of textual data corpora is required. After researching large datasets of Tweets, Twitter Sentiment Dataset on Kaggle was proven to be the best for this project. With 163 thousand tweets extracted using the Twitter API, the authors have categorised each tweet to have either a positive, neutral or negative sentiment, which is beneficial for the algorithm in categorising harmful texts.\n",
    "\n",
    "The dataset consists of the clean_text (defined as the Tweets that do not have redundant information) and the sentiment of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f381e0e6",
   "metadata": {},
   "source": [
    "### 1.4 Evaluation methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d3b22",
   "metadata": {},
   "source": [
    "## II. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f69e39",
   "metadata": {},
   "source": [
    "### 2.1 Preparation of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38466cb4",
   "metadata": {},
   "source": [
    "#### Acquiring dataset\n",
    "The dataset on the collection of Tweets were acquired from Kaggle by downloading the CSV file. The author of this dataset is Saurabh Shahane. The code for importing the dataset is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641807df",
   "metadata": {},
   "source": [
    "#### Importing libraries\n",
    "- <b>pandas library</b> was imported to process and handle datasets in Python. It is used to help write and read from CSV files while handling real-world messy data and processing them into a proper format\n",
    "\n",
    "- <b>numpy library</b> was imported to handle calculations and use numpy arrays for statistical calculations\n",
    "\n",
    "- <b>matplotlib library</b> was imported to plot the data and represent it graphically\n",
    "\n",
    "- <b>os library</b> was imported to have a way of using the operating system dependent functionalities, more specifically to save the dataset as a CSV\n",
    "\n",
    "- <b>stopwords library</b> was imported to have a library of the most common words in data to aid in stopwords removal\n",
    "\n",
    "- <b>collections.Counter library</b> is a dictionary subclass that counts the frequency of elements. This helps to check for additional common words being used\n",
    "\n",
    "- <b>ast library</b> was imported to convert a string that looked like a list to an actual list\n",
    "\n",
    "- <b>WordNetLemmatizer library</b> was imported to perform lemmatisation on words. It reduces words to its base or dictionary form, known as the lemma\n",
    "\n",
    "- <b>nltk.corpus library</b> was used to access contents of a diverse set of corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26cf64bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following block of code was self-written\n",
    "# dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# text processing and analysis\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# analysing text corpora\n",
    "from collections import Counter\n",
    "\n",
    "# modifying string to list\n",
    "import ast\n",
    "# stemming and lemmatisation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# regular expressions\n",
    "import re\n",
    "\n",
    "# lexical analysis\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e59f38",
   "metadata": {},
   "source": [
    "#### Helper function\n",
    "As the dataset will be saved at each section for easier access to the CSV based on each progressive step, the saving step will be changed to a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efd07a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following block of code was self-written\n",
    "def save_to_csv(df,file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        df.to_csv(file_path, index = False)\n",
    "        print('File saved successfully.')\n",
    "    else:\n",
    "        print('File already exists.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39c4f8",
   "metadata": {},
   "source": [
    "#### Importing dataset\n",
    "To check that the dataset is ready for cleaning and analysis, we will look at the first entry to check if there are headers. Since there are headers, and the dataset contains only relevant information, the text, and sentiment, the headers will just be modified to \"tweet\" and \"sentiment\" for better comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6bb523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following block of code was self-written\n",
    "df = pd.read_csv('datasets/Twitter_Data_Sentiments.csv', nrows = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b98626b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following block of code was self-written\n",
    "tweets_df = pd.read_csv('datasets/Twitter_Data_Sentiments.csv')\n",
    "tweets_df.columns = ['tweet', 'sentiment']\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e1df4",
   "metadata": {},
   "source": [
    "### 2.2 Pre-processing\n",
    "Completed steps:\n",
    "- Removing NaN, infinite and duplicated entries\n",
    "- Reducing sample size, storing locally\n",
    "- Pre-processing (basic text processing)\n",
    "    - Removal of stop words + tokenisation\n",
    "    - Frequency distribution (removal of additional stop words)\n",
    "    - Lemmatisation\n",
    "    - Lexical diversity + frequency distribution\n",
    "    \n",
    "(writeup not needed)\n",
    "\n",
    "TO DO LIST:\n",
    "- Convert/store the dataset locally and preprocess the data [**done**]\n",
    "- Describe the text representation (e.g., bag of words, word embedding, etc.) **[not done]**\n",
    "- any pre-processing steps you have applied and why they were needed (e.g. tokenisation [**done**], lemmatization [**done**])\n",
    "- Describe the vocabulary and file type/format, e.g. CSV file. [**not done**]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf3b6af",
   "metadata": {},
   "source": [
    "#### Removing NaN or infinite entries\n",
    "To ensure that the dataset contains only required information, we will check and remove any entries that contain missing or infinite values.\n",
    "\n",
    "#### Removing duplicated entries\n",
    "To ensure that the analysis is beneficial, all entriesshould be unique. A 'duplicated' column will be added to the a temporary copy of the dataset which is the output of the duplicated() function and we will print only columns where the 'duplicated' column is True. Based on the output, it is seen that there are no duplicated Tweets.\n",
    "\n",
    "#### Reducing sample size\n",
    "To address computational constraints and the imbalanced nature of the dataset (positives to negatives having an approximate ratio of 2:1), the analysis will be limited to a subset of 1000 instances for each sentiment category.\n",
    "Due to the limitations of computer capacity and the imbalanced nature of the dataset, we will limit the analysis to a subset of 1000 instances for each sentiment category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d828a0aa",
   "metadata": {},
   "source": [
    "<b>The following blocks of code was self-written with references</b>\n",
    "<br>replace() function: https://sparkbyexamples.com/pandas/pandas-drop-infinite-values-from-dataframe/?expand_article=1\n",
    "<br>checking for duplicates: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html\n",
    "<br>creation of sample from large dataframe: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b8652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following block of code was self-written\n",
    "sentiments = tweets_df['sentiment'].value_counts(dropna = False)\n",
    "infinites = np.isinf(tweets_df['sentiment']).sum()\n",
    "sentiments['Infinite'] = infinites\n",
    "print(\"sentiments before cleaning:\\n\\n\", sentiments)\n",
    "\n",
    "# Removing NaN values\n",
    "tweets_df.dropna(inplace = True)\n",
    "# Replacing and removing infinite values\n",
    "tweets_df.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "tweets_df.dropna(inplace = True, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "842d885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following block of code was self-written\n",
    "dupe_checker = tweets_df.copy()\n",
    "duplicates = dupe_checker.duplicated()\n",
    "dupe_checker['duplicated'] = duplicates\n",
    "duplicated = dupe_checker[dupe_checker['duplicated'] == True]\n",
    "duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "249da25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following block of code was self-written\n",
    "# Storing 1000 entries of each sentiment\n",
    "positives = tweets_df[tweets_df['sentiment'] == 1].sample(n = 1000, random_state = 10)\n",
    "neutrals = tweets_df[tweets_df['sentiment'] == 0].sample(n = 1000, random_state = 10)\n",
    "negatives = tweets_df[tweets_df['sentiment'] == -1].sample(n = 1000, random_state = 10)\n",
    "\n",
    "# Concatenating the 3 sentiments together\n",
    "sampled_tweets_df = pd.concat([positives, neutrals, negatives])\n",
    "# Resetting index\n",
    "sampled_tweets_df.reset_index(drop = True, inplace = True)\n",
    "print('positive sentiments:', sampled_tweets_df[sampled_tweets_df['sentiment'] == 1].shape[0])\n",
    "print('neutral sentiments:', sampled_tweets_df[sampled_tweets_df['sentiment'] == 0].shape[0])\n",
    "print('negative sentiments:', sampled_tweets_df[sampled_tweets_df['sentiment'] == -1].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b439f3",
   "metadata": {},
   "source": [
    "The tweets_df will now be saved into a new dataset for easier accessibility. To ensure that no duplicates are saved, a simple path checking will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e132cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following block of code was self-written\n",
    "filename = 'datasets/sample3000_Twitter_Data_Sentiments.csv'\n",
    "save_to_csv(sampled_tweets_df ,filename)\n",
    "tweets_df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58efb49a",
   "metadata": {},
   "source": [
    "#### Basic text processing\n",
    "To begin the process of analysing the text, we would require conducting the following basic text processing methods:\n",
    "- Removal of stop words + tokenisation\n",
    "- Frequency distribution (removal of additional stop words)\n",
    "- Lemmatisation\n",
    "- Lexical diversity + frequency distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac1183",
   "metadata": {},
   "source": [
    "- <b>Removing stop words</b>: In human language, it is very common for stop words to be present. These words, including **determiners** (eg: the, a, this, my), **conjunctions** (eg: and, or, nor, but, whereas) and **prepositions** (eg: against, along, at, before), are used to connect thoughts and speech to form grammatically accurate sentences or structural cohesion. While important during communication amongst one another, they do not carry importance or sentiments that would be valuable to this project, thus introducing noise. The removal would help to streamline the process to focus on words that would contribute more meaning to the sentiment of Tweets.\n",
    "\n",
    "* tokenisation will be done in lowercase as all stopwords are in lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a1d0d",
   "metadata": {},
   "source": [
    "<b>The following blocks of code was self-written with reference</b>\n",
    "<br>stopwords: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bd5d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following block of code was self-written\n",
    "# Downloading stopword corpus\n",
    "nltk.download('stopwords')\n",
    "# Get stopword list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Checking removal works on a test text\n",
    "test_tweet = 'This is a test that Stopword removal works.'\n",
    "\n",
    "tokens = test_tweet.lower().split()\n",
    "# Removing each token if part of stop_words\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "print(\"Filtered from\", len(tokens), \"to\", len(filtered_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f2eb9",
   "metadata": {},
   "source": [
    "The following commented out code was the original self-written code that now is not being used.\n",
    "<br>It was modified to aid in the stemming and lemmatisation in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36b231fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following block of code was self-written\n",
    "tweets = tweets_df['tweet'].tolist()\n",
    "\n",
    "filtered_tweets = []\n",
    "for tweet in tweets:\n",
    "    tokens = tweet.lower().split()\n",
    "    # Removing each token if part of stop_words\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    filtered_tweets.append(filtered_tokens)\n",
    "\n",
    "tweets_df['filtered_tweet'] = filtered_tweets\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3702e",
   "metadata": {},
   "source": [
    "- <b>Regular expressions (Regex)</b>: consider if i need this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0871932",
   "metadata": {},
   "source": [
    "The removal of stopwords has reduced the texts. Due to the dataset chosen having the column named as \"clean_text\", as well as the analysis from the above output having highly unique text, the text will now be examined.\n",
    "\n",
    "#### Frequency distribution\n",
    "By making use of the **collections.Counter library**, it would allow the most used words to be displayed. Due to the word \"modi\" being seen 2831/3000 times, this would be considered a stop word. Words containing the substring \"modi\" will be removed from all entries to ensure that all representations of this stop word would not be part of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1efee",
   "metadata": {},
   "source": [
    "<b>The following blocks of code was self-written with reference</b>\n",
    "<br>collections library: https://www.digitalocean.com/community/tutorials/python-counter-python-collections-counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "105c03c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following block of code was self-written\n",
    "# Concatenate all tokenised words together\n",
    "tokenised_words = [word for tweet in tweets_df['filtered_tweet'] for word in tweet]\n",
    "# Count word frequency\n",
    "word_counts = Counter(tokenised_words)\n",
    "\n",
    "# Displays top 10 used words and their frequencies\n",
    "most_used_words = word_counts.most_common(5)\n",
    "for word, count in most_used_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a7282d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following block of code was self-written\n",
    "# Iterating through rows to check for target word\n",
    "for index, row in tweets_df.iterrows():\n",
    "    tweet_words = row['filtered_tweet']\n",
    "    tweet_words = [word for word in tweet_words if 'modi' not in word]\n",
    "    tweets_df.at[index, 'filtered_tweet'] = tweet_words\n",
    "\n",
    "# Check that stopword \"modi\" is not part of frequently used words\n",
    "tokenized_words = [word for tweet in tweets_df['filtered_tweet'] for word in tweet]\n",
    "word_counts = Counter(tokenized_words)\n",
    "most_used_words = word_counts.most_common(5)\n",
    "for word, frequency in most_used_words:\n",
    "    print(f\"{word}: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a547f64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The following block of code was self-written\n",
    "# Get top 30 most used words\n",
    "most_used_words = word_counts.most_common(30)\n",
    "# Extract the words and their frequencies\n",
    "words, frequencies = zip(*most_used_words)\n",
    "\n",
    "# Plot the word frequencies\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.bar(words, frequencies)\n",
    "\n",
    "# Labelling and title\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 30 Words Most Frequently Used')\n",
    "# Rotate x-axis labels if needed\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e62dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = tweets_df.copy()\n",
    "nan_entries = copy[copy.isna().any(axis=1)]\n",
    "nan_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ece79d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "# The following block of code was self-written\n",
    "filename = 'datasets/tokenised_Twitter_Data_Sentiments.csv'\n",
    "save_to_csv(tweets_df, filename)\n",
    "tweets_df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22387e7c",
   "metadata": {},
   "source": [
    "#### Lemmatisation\n",
    "The first step before lemmatisation is to ensure that the input is a string, as the column filtered_tweets seem to be a string looking like a list, we would have to resolve that issue before starting lemmatisation.\n",
    "\n",
    "** writeup on lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc61ed6b",
   "metadata": {},
   "source": [
    "<b>The following blocks of code was self-written with reference</b>\n",
    "<br>literal_eval: https://docs.python.org/3/library/ast.html\n",
    "<br>understanding lemmatisation: https://www.geeksforgeeks.org/python-lemmatization-with-nltk/\n",
    "<br>regular expression: https://datascienceparichay.com/article/python-keep-only-letters-from-a-string/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f974fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>filtered_tweet</th>\n",
       "      <th>lemmatized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>infinity cant chased two nations theory has be...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['infinity', 'cant', 'chased', 'two', 'nations...</td>\n",
       "      <td>infinity cant chased two nation theory practic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>years dynamic modi rule and fear losing electi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['years', 'dynamic', 'rule', 'fear', 'losing',...</td>\n",
       "      <td>year dynamic rule fear losing election compell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>terrorists pakistan want lose opposition win m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['terrorists', 'pakistan', 'want', 'lose', 'op...</td>\n",
       "      <td>terrorist pakistan want lose opposition win in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the entire panel opposition panel what can exp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['entire', 'panel', 'opposition', 'panel', 'ex...</td>\n",
       "      <td>entire panel opposition panel expect debate co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theres one disgustinggundaghaleezghatyascum yo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>['theres', 'one', 'disgustinggundaghaleezghaty...</td>\n",
       "      <td>there one disgustinggundaghaleezghatyascum tah...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment  \\\n",
       "0  infinity cant chased two nations theory has be...        1.0   \n",
       "1  years dynamic modi rule and fear losing electi...        1.0   \n",
       "2  terrorists pakistan want lose opposition win m...        1.0   \n",
       "3  the entire panel opposition panel what can exp...        1.0   \n",
       "4  theres one disgustinggundaghaleezghatyascum yo...        1.0   \n",
       "\n",
       "                                      filtered_tweet  \\\n",
       "0  ['infinity', 'cant', 'chased', 'two', 'nations...   \n",
       "1  ['years', 'dynamic', 'rule', 'fear', 'losing',...   \n",
       "2  ['terrorists', 'pakistan', 'want', 'lose', 'op...   \n",
       "3  ['entire', 'panel', 'opposition', 'panel', 'ex...   \n",
       "4  ['theres', 'one', 'disgustinggundaghaleezghaty...   \n",
       "\n",
       "                                    lemmatized_tweet  \n",
       "0  infinity cant chased two nation theory practic...  \n",
       "1  year dynamic rule fear losing election compell...  \n",
       "2  terrorist pakistan want lose opposition win in...  \n",
       "3  entire panel opposition panel expect debate co...  \n",
       "4  there one disgustinggundaghaleezghatyascum tah...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following block of code was self-written\n",
    "# Initialize lemmatiser\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Iterate through the rows of the dataframe\n",
    "for index, row in tweets_df.iterrows():\n",
    "    set_of_words = row['filtered_tweet']\n",
    "    \n",
    "    # Convert the set of words into a string\n",
    "    word_list = ' '.join(set_of_words)\n",
    "    word_list = word_list.replace(' ', '')\n",
    "    word_list = ast.literal_eval(word_list)\n",
    "    word_string = ' '.join(word_list)\n",
    "    \n",
    "    # Perform lemmatization on the stemmed string\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in word_string.split()]\n",
    "    lemmatized_string = ' '.join(lemmatized_words)\n",
    "    \n",
    "    # Update the dataframe with the lemmatized string\n",
    "    tweets_df.at[index, 'lemmatized_tweet'] = lemmatized_string\n",
    "    \n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a285ebf1",
   "metadata": {},
   "source": [
    "#### Regular expression\n",
    "removes any non alphabeticals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "843e4a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing that is removed\n"
     ]
    }
   ],
   "source": [
    "# The following block of code was self-written\n",
    "def clean_text_input(text):\n",
    "    cleaned_text = re.sub(r\"\\b\\d+\\b\\s*\", '', text)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "print(clean_text_input('testing that 200000 is removed'))\n",
    "tweets_df['regex_tweet'] = tweets_df['lemmatized_tweet'].apply(clean_text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11438188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "# The following block of code was self-written\n",
    "filename = 'datasets/cleaned_Twitter_Data_Sentiments.csv'\n",
    "save_to_csv(tweets_df, filename)\n",
    "tweets_df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257df8d",
   "metadata": {},
   "source": [
    "#### Lexical diversity\n",
    "Lexical analysis is used as a metric to measure the variety or richness of the vocabulary used in a text corpora. This helps to gain understanding to the complexity of the data being used.\n",
    "\n",
    "$$\\text{Lexical Diversity} = \\frac{\\text{Total Number of Unique Words}}{\\text{Total Number of Words}}$$\n",
    "\n",
    "Based on the output, the language used in both positive and negative sentiments is slightly less diverse compared to the neutral sentiments. However, since the difference is minute, it would suggest that the vocabulary used and linguistic complexity are similar across all sentiments. This is a valid statement as the dataset consists of already modified tweets by the owner of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17a585bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unique_total_tokens \u001b[38;5;241m/\u001b[39m total_tokens\n\u001b[0;32m     21\u001b[0m positive_lexical_diversity \u001b[38;5;241m=\u001b[39m calc_lexicalDiversity(positive_tokens)\n\u001b[1;32m---> 22\u001b[0m neutral_lexical_diversity \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_lexicalDiversity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneutral_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m negative_lexical_diversity \u001b[38;5;241m=\u001b[39m calc_lexicalDiversity(negative_tokens)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive lexical diversity:\u001b[39m\u001b[38;5;124m\"\u001b[39m, positive_lexical_diversity)\n",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m, in \u001b[0;36mcalc_lexicalDiversity\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m     12\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Counts all tokens\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m total_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Creates set of unique tokens\u001b[39;00m\n\u001b[0;32m     16\u001b[0m unique_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(token \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sublist)\n",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token]\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Counts all tokens\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m total_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Creates set of unique tokens\u001b[39;00m\n\u001b[0;32m     16\u001b[0m unique_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(token \u001b[38;5;28;01mfor\u001b[39;00m sublist \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m sublist)\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "# The following block of code was self-written\n",
    "positive_sentiments = tweets_df[tweets_df['sentiment'] == 1]\n",
    "neutral_sentiments = tweets_df[tweets_df['sentiment'] == 0]\n",
    "negative_sentiments = tweets_df[tweets_df['sentiment'] == -1]\n",
    "\n",
    "positive_tokens = positive_sentiments['regex_tweet'].str.split()\n",
    "neutral_tokens = neutral_sentiments['regex_tweet'].str.split()\n",
    "negative_tokens = negative_sentiments['regex_tweet'].str.split()\n",
    "\n",
    "def calc_lexicalDiversity(tokens):\n",
    "    # Exclude empty string from tokens\n",
    "    tokens = [token for token in tokens if token]\n",
    "    # Counts all tokens\n",
    "    total_tokens = sum(len(token) for token in tokens)\n",
    "    # Creates set of unique tokens\n",
    "    unique_tokens = set(token for sublist in tokens for token in sublist)\n",
    "    # Counts set of unique tokens\n",
    "    unique_total_tokens = len(unique_tokens)\n",
    "    return unique_total_tokens / total_tokens\n",
    "\n",
    "positive_lexical_diversity = calc_lexicalDiversity(positive_tokens)\n",
    "neutral_lexical_diversity = calc_lexicalDiversity(neutral_tokens)\n",
    "negative_lexical_diversity = calc_lexicalDiversity(negative_tokens)\n",
    "\n",
    "print(\"positive lexical diversity:\", positive_lexical_diversity)\n",
    "print(\"neutral lexical diversity:\", neutral_lexical_diversity)\n",
    "print(\"negative lexical diversity:\", negative_lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2fd548",
   "metadata": {},
   "source": [
    "### 2.3 Baseline performance\n",
    "(writeup not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ca830",
   "metadata": {},
   "source": [
    "### 2.4 Classification approach\n",
    "(writeup not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d0d9cc",
   "metadata": {},
   "source": [
    "### 2.5 Coding style\n",
    "(writeup not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c187d",
   "metadata": {},
   "source": [
    "## III. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c2562",
   "metadata": {},
   "source": [
    "### 3.1 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e3055",
   "metadata": {},
   "source": [
    "### 3.2 Summary and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950434fd",
   "metadata": {},
   "source": [
    "## Temporary reference list\n",
    "* to use citation generator\n",
    "\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
