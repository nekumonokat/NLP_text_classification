{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d423c27a",
   "metadata": {},
   "source": [
    "<h2><center>NLP Text Classification</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d0067",
   "metadata": {},
   "source": [
    "## I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101e842",
   "metadata": {},
   "source": [
    "### 1.1 Domain-specific area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72286743",
   "metadata": {},
   "source": [
    "### 1.2 Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd0cf1",
   "metadata": {},
   "source": [
    "### 1.3 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f381e0e6",
   "metadata": {},
   "source": [
    "### 1.4 Evaluation methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d3b22",
   "metadata": {},
   "source": [
    "## II. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e1df4",
   "metadata": {},
   "source": [
    "### 2.1 Pre-processing\n",
    "(writeup not needed)\n",
    "<br>**to be removed**: Convert/store the dataset locally and preprocess the data. Describe the text representation\n",
    "(e.g., bag of words, word embedding, etc.) and any pre-processing steps you have applied\n",
    "and why they were needed (e.g. tokenization, lemmatization). Describe the vocabulary and\n",
    "file type/format, e.g. CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38466cb4",
   "metadata": {},
   "source": [
    "#### Acquiring dataset\n",
    "The dataset on the collection of Tweets were acquired from Kaggle by downloading the CSV file. The author of this dataset is Saurabh Shahane. The code for importing the dataset is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641807df",
   "metadata": {},
   "source": [
    "#### Importing libraries\n",
    "- <b>pandas library</b> was imported to process and handle datasets in Python. It is used to help write and read from CSV files while handling real-world messy data and processing them into a proper format\n",
    "\n",
    "- <b>numpy library</b> was imported to handle calculations and use numpy arrays for statistical calculations\n",
    "\n",
    "- <b>matplotlib library</b> was imported to plot the data and represent it graphically [not used]\n",
    "\n",
    "- <b>os library</b> was imported to have a way of using the operating system dependent functionalities, more specifically to save the dataset as a CSV\n",
    "\n",
    "- <b>stopwords library</b> was imported to have a library of the most common words in data to aid in stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "26cf64bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sbgka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# text processing and analysis\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# analysing text corpora\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39c4f8",
   "metadata": {},
   "source": [
    "#### Importing dataset\n",
    "To check that the dataset is ready for cleaning and analysis, we will look at the first entry to check if there are headers. Since there are headers, and the dataset contains only relevant information, the text, and sentiment, the headers will just be modified to \"tweet\" and \"sentiment\" for better comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6c6bb523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  category\n",
       "0  when modi promised “minimum government maximum...        -1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following block of code was self-written\n",
    "df = pd.read_csv('datasets/Twitter_Data_Sentiments.csv', nrows = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b98626b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment\n",
       "0  when modi promised “minimum government maximum...       -1.0\n",
       "1  talk all the nonsense and continue all the dra...        0.0\n",
       "2  what did just say vote for modi  welcome bjp t...        1.0\n",
       "3  asking his supporters prefix chowkidar their n...        1.0\n",
       "4  answer who among these the most powerful world...        1.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following block of code was self-written\n",
    "tweets_df = pd.read_csv('datasets/Twitter_Data_Sentiments.csv')\n",
    "tweets_df.columns = ['tweet', 'sentiment']\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8302410",
   "metadata": {},
   "source": [
    "#### Removing NaN or infinite entries\n",
    "To ensure that the dataset contains only required information, we will check and remove any entries that contain missing or infinite values.\n",
    "\n",
    "#### Removing duplicated entries\n",
    "To ensure that the analysis is beneficial, all entriesshould be unique. A 'duplicated' column will be added to the a temporary copy of the dataset which is the output of the duplicated() function and we will print only columns where the 'duplicated' column is True. Based on the output, it is seen that there are no duplicated Tweets.\n",
    "\n",
    "#### Reducing sample size\n",
    "To address computational constraints and the imbalanced nature of the dataset (positives to negatives having an approximate ratio of 2:1), the analysis will be limited to a subset of 1000 instances for each sentiment category.\n",
    "Due to the limitations of computer capacity and the imbalanced nature of the dataset, we will limit the analysis to a subset of 1000 instances for each sentiment category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89674727",
   "metadata": {},
   "source": [
    "<b>The following blocks of code was self-written with references</b>\n",
    "<br>replace() function: https://sparkbyexamples.com/pandas/pandas-drop-infinite-values-from-dataframe/?expand_article=1\n",
    "<br>checking for duplicates: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html\n",
    "<br>creation of sample from large dataframe: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ef526aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiments before cleaning:\n",
      "\n",
      " sentiment\n",
      "1.0         72250\n",
      "0.0         55213\n",
      "-1.0        35510\n",
      "NaN             7\n",
      "Infinite        0\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sentiments = tweets_df['sentiment'].value_counts(dropna = False)\n",
    "infinites = np.isinf(tweets_df['sentiment']).sum()\n",
    "sentiments['Infinite'] = infinites\n",
    "print(\"sentiments before cleaning:\\n\\n\", sentiments)\n",
    "\n",
    "# Removing NaN values\n",
    "tweets_df.dropna(inplace = True)\n",
    "# Replacing and removing infinite values\n",
    "tweets_df.replace([np.inf, -np.inf], np.nan, inplace = True)\n",
    "tweets_df.dropna(inplace = True, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1efcaef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>duplicated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tweet, sentiment, duplicated]\n",
       "Index: []"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dupe_checker = tweets_df.copy()\n",
    "duplicates = dupe_checker.duplicated()\n",
    "dupe_checker['duplicated'] = duplicates\n",
    "duplicated = dupe_checker[dupe_checker['duplicated'] == True]\n",
    "duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0e3e08c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive sentiments: 1000\n",
      "neutral sentiments: 1000\n",
      "negative sentiments: 1000\n"
     ]
    }
   ],
   "source": [
    "# Storing 1000 entries of each sentiment\n",
    "positives = tweets_df[tweets_df['sentiment'] == 1].sample(n = 1000, random_state = 10)\n",
    "neutrals = tweets_df[tweets_df['sentiment'] == 0].sample(n = 1000, random_state = 10)\n",
    "negatives = tweets_df[tweets_df['sentiment'] == -1].sample(n = 1000, random_state = 10)\n",
    "\n",
    "# Concatenating the 3 sentiments together\n",
    "sampled_tweets_df = pd.concat([positives, neutrals, negatives])\n",
    "# Resetting index\n",
    "sampled_tweets_df.reset_index(drop = True, inplace = True)\n",
    "print('positive sentiments:', sampled_tweets_df[sampled_tweets_df['sentiment'] == 1].shape[0])\n",
    "print('neutral sentiments:', sampled_tweets_df[sampled_tweets_df['sentiment'] == 0].shape[0])\n",
    "print('negative sentiments:', sampled_tweets_df[sampled_tweets_df['sentiment'] == -1].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b439f3",
   "metadata": {},
   "source": [
    "The tweets_df will now be saved into a new dataset for easier accessibility. To ensure that no duplicates are saved, a simple path checking will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e132cf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = 'datasets/sample3000_Twitter_Data_Sentiments.csv'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    sampled_tweets_df.to_csv(file_path, index = False)\n",
    "    print('File saved successfully.')\n",
    "else:\n",
    "    print('File already exists.')\n",
    "   \n",
    "tweets_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58efb49a",
   "metadata": {},
   "source": [
    "#### Basic text processing\n",
    "To begin the process of analysing the text, we would require conducting basic text processing methods.\n",
    "\n",
    "TO DO LIST:\n",
    "- Describe the text representation (e.g., bag of words, word embedding, etc.) **[not done]**\n",
    "- Describe the vocabulary and file type/format, e.g. CSV file. [**not done**]\n",
    "- any pre-processing steps you have applied and why they were needed (e.g. tokenization [**done**], lemmatization [**did regex**])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac1183",
   "metadata": {},
   "source": [
    "- <b>Removing stop words</b>: In human language, it is very common for stop words to be present. These words, including **determiners** (eg: the, a, this, my), **conjunctions** (eg: and, or, nor, but, whereas) and **prepositions** (eg: against, along, at, before), are used to connect thoughts and speech to form grammatically accurate sentences or structural cohesion. While important during communication amongst one another, they do not carry importance or sentiments that would be valuable to this project, thus introducing noise. The removal would help to streamline the process to focus on words that would contribute more meaning to the sentiment of Tweets.\n",
    "\n",
    "* tokenization will be done in lowercase as all stopwords are in lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a83f5a",
   "metadata": {},
   "source": [
    "<b>The following blocks of code was self-written with reference</b>\n",
    "<br>stopwords: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2bd5d87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered from 8 to 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sbgka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Downloading stopword corpus\n",
    "nltk.download('stopwords')\n",
    "# Get stopword list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Checking removal works on a test text\n",
    "test_tweet = 'This is a test that Stopword removal works.'\n",
    "\n",
    "tokens = test_tweet.lower().split()\n",
    "# Removing each token if part of stop_words\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "print(\"Filtered from\", len(tokens), \"to\", len(filtered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9fe2801f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>filtered_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>infinity cant chased two nations theory has be...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[infinity, cant, chased, two, nations, theory,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>years dynamic modi rule and fear losing electi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[years, dynamic, modi, rule, fear, losing, ele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>terrorists pakistan want lose opposition win m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[terrorists, pakistan, want, lose, opposition,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the entire panel opposition panel what can exp...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[entire, panel, opposition, panel, expect, deb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>theres one disgustinggundaghaleezghatyascum yo...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[theres, one, disgustinggundaghaleezghatyascum...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  sentiment  \\\n",
       "0  infinity cant chased two nations theory has be...        1.0   \n",
       "1  years dynamic modi rule and fear losing electi...        1.0   \n",
       "2  terrorists pakistan want lose opposition win m...        1.0   \n",
       "3  the entire panel opposition panel what can exp...        1.0   \n",
       "4  theres one disgustinggundaghaleezghatyascum yo...        1.0   \n",
       "\n",
       "                                      filtered_tweet  \n",
       "0  [infinity, cant, chased, two, nations, theory,...  \n",
       "1  [years, dynamic, modi, rule, fear, losing, ele...  \n",
       "2  [terrorists, pakistan, want, lose, opposition,...  \n",
       "3  [entire, panel, opposition, panel, expect, deb...  \n",
       "4  [theres, one, disgustinggundaghaleezghatyascum...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = tweets_df['tweet'].tolist()\n",
    "\n",
    "filtered_tweets = []\n",
    "for tweet in tweets:\n",
    "    tokens = tweet.lower().split()\n",
    "    # Removing each token if part of stop_words\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    filtered_tweets.append(filtered_tokens)\n",
    "\n",
    "tweets_df['filtered_tweet'] = filtered_tweets\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3702e",
   "metadata": {},
   "source": [
    "- <b>Regular expressions (Regex)</b>: consider if i need this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0871932",
   "metadata": {},
   "source": [
    "The removal of stopwords has reduced the texts. Due to the dataset chosen having the column named as \"clean_text\", as well as the analysis from the above output having highly unique text, the text will now be examined.\n",
    "\n",
    "#### Evaluation of words\n",
    "1. By making use of the collections.Counter library, it would allow the most used words to be displayed. Due to the word \"modi\" being seen 2831/3000 times, this would be considered a stop word. The word \"modi\" will be removed from all entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb18fad",
   "metadata": {},
   "source": [
    "<b>The following blocks of code was self-written with reference</b>\n",
    "<br>collections library: https://www.digitalocean.com/community/tutorials/python-counter-python-collections-counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5c24013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modi: 2831\n",
      "india: 499\n",
      "bjp: 253\n",
      "congress: 231\n",
      "people: 230\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all tokenized words together\n",
    "tokenized_words = [word for tweet in tweets_df['filtered_tweet'] for word in tweet]\n",
    "# Count word frequency\n",
    "word_counts = Counter(tokenized_words)\n",
    "\n",
    "# Displays top 10 used words and their frequencies\n",
    "most_used_words = word_counts.most_common(5)\n",
    "for word, count in most_used_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7e1d57de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india: 499\n",
      "bjp: 253\n",
      "congress: 231\n",
      "people: 230\n",
      "like: 222\n"
     ]
    }
   ],
   "source": [
    "# Iterating through rows to check for target word\n",
    "for index, row in tweets_df.iterrows():\n",
    "    tweet_words = row['filtered_tweet']\n",
    "    tweet_words = [word for word in tweet_words if word != 'modi']\n",
    "    tweets_df.at[index, 'filtered_tweet'] = tweet_words\n",
    "    \n",
    "# Check that stopword \"modi\" is not part of frequently used words\n",
    "tokenized_words = [word for tweet in tweets_df['filtered_tweet'] for word in tweet]\n",
    "word_counts = Counter(tokenized_words)\n",
    "most_used_words = word_counts.most_common(5)\n",
    "for word, count in most_used_words:\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2fd548",
   "metadata": {},
   "source": [
    "### 2.2 Baseline performance\n",
    "(writeup not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ca830",
   "metadata": {},
   "source": [
    "### 2.3 Classification approach\n",
    "(writeup not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d0d9cc",
   "metadata": {},
   "source": [
    "### 2.4 Coding style\n",
    "(writeup not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c187d",
   "metadata": {},
   "source": [
    "## III. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c2562",
   "metadata": {},
   "source": [
    "### 3.1 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e3055",
   "metadata": {},
   "source": [
    "### 3.2 Summary and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950434fd",
   "metadata": {},
   "source": [
    "## Temporary reference list\n",
    "* to use citation generator\n",
    "\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
