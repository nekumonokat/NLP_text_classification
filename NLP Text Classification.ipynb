{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d423c27a",
   "metadata": {},
   "source": [
    "<h2><center>NLP Text Classification</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b5544d",
   "metadata": {},
   "source": [
    "## I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb3ab8",
   "metadata": {},
   "source": [
    "### 1.1 Domain-specific area\n",
    "This project provides an analysis of textual data on Twitter to accurately detect and classify threatening or harmful content using sentiment analysis techniques. This would provide the cybersecurity industry a tool that takes in a corpus of text for training to develop a strong detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761f8e8",
   "metadata": {},
   "source": [
    "### 1.2 Objectives\n",
    "Due to popular algorithms being centered around the detection of cyberbullying on social media (Cynthia Van Hee et al., 2018), it is important for this project to widen the scope of detection. While the general detection algorithms focus mainly on terrorism and cyberbullying, it is a known fact that cybersecurity encompasses more than those 2 focuses. (Khairy, Mahmoud and Abd-El-Hafeez, 2021) While full security and safety of users cannot be ensured, making these adjustments would contribute valuable insights for future development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2a0c9",
   "metadata": {},
   "source": [
    "### 1.3 Dataset\n",
    "To begin this project, an extensive amount of textual data corpora is required. After researching large datasets of Tweets, Sentiment140 Kaggle was proven to be the best for this project. With 1.6 million tweets extracted using the Twitter API, the authors have categorised each tweet to have either a positive, neutral or negative sentiment, which is beneficial for the algorithm in categorising harmful texts.\n",
    "\n",
    "The dataset consists of the target (defined as the sentiment of the text), the tweet IDs, date, flags (possible queries, which would be removed in the initialisation phase of extracting the data), the username, and the text of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4796f8ae",
   "metadata": {},
   "source": [
    "### 1.4 Evaluation methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c872297b",
   "metadata": {},
   "source": [
    "## II. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31657960",
   "metadata": {},
   "source": [
    "### 2.1 Pre-processing\n",
    "(writeup not needed)\n",
    "<br>Convert/store the dataset locally and preprocess the data. Describe the text representation\n",
    "(e.g., bag of words, word embedding, etc.) and any pre-processing steps you have applied\n",
    "and why they were needed (e.g. tokenization, lemmatization). Describe the vocabulary and\n",
    "file type/format, e.g. CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaeb162",
   "metadata": {},
   "source": [
    "#### Acquiring dataset\n",
    "The dataset on the collection of Tweets were acquired from Kaggle by downloading the CSV file. The author of this dataset is Μαριος Μιχαηλιδης KazAnova. The code for importing the dataset is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050a1e1f",
   "metadata": {},
   "source": [
    "#### 2.1.1 Importing libraries\n",
    "- <b>pandas library</b> was imported to process and handle dataframes in Python. It is used to help write and read from CSV files while handling real-world messy data and processing them into a proper format\n",
    "\n",
    "- <b>numpy library</b> was imported to handle calculations and use numpy arrays for statistical calculations\n",
    "\n",
    "- <b>matplotlib library</b> was imported to plot the data and represent it graphically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ada43af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d68f8d",
   "metadata": {},
   "source": [
    "#### 2.1.2 Creating helper functions\n",
    "- <b>displaysetsH</b> takes in a list of datasets and an optional number of rows to display the head of each dataset\n",
    "\n",
    "- <b>displaysetsT</b> takes in a list of datasets and an optional number of rows to display the tail of each dataset\n",
    "\n",
    "- <b>resetidx</b> takes in a list of datasets to reset the indexes of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590e3931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displaysetsH(datasets, amt = 5):\n",
    "    for dataset in datasets:\n",
    "        display(dataset.head(amt))\n",
    "        \n",
    "def displaysetsT(datasets, amt = 5):\n",
    "    for dataset in datasets:\n",
    "        display(dataset.head(amt))\n",
    "        \n",
    "def resetidx(datasets):\n",
    "    for dataset in datasets:\n",
    "        dataset.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39665da0",
   "metadata": {},
   "source": [
    "#### 2.1.3 Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3c2ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75be3859",
   "metadata": {},
   "source": [
    "### 2.2 Baseline performance\n",
    "(writeup not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31462f46",
   "metadata": {},
   "source": [
    "### 2.3 Classification approach\n",
    "(writeup not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a835d7",
   "metadata": {},
   "source": [
    "### 2.4 Coding style\n",
    "(writeup not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac8da7",
   "metadata": {},
   "source": [
    "## III. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efabf54",
   "metadata": {},
   "source": [
    "### 3.1 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad7750",
   "metadata": {},
   "source": [
    "### 3.2 Summary and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60481c3b",
   "metadata": {},
   "source": [
    "## Temporary reference list\n",
    "* to use citation generator\n",
    "\n",
    "- Cynthia Van Hee, Jacobs, G., Emmery, C., Desmet, B., Lefever, E., Verhoeven, B., Guy De Pauw, Daelemans, W. and Hoste, V. (2018). Automatic detection of cyberbullying in social media text. PLOS ONE, [online] 13(10), p.e0203794. doi:https://doi.org/10.1371/journal.pone.0203794.\n",
    "- Khairy, M., Mahmoud, T.M. and Abd-El-Hafeez, T. (2021). Automatic Detection of Cyberbullying and Abusive Language in Arabic Content on Social Networks: A Survey. Procedia Computer Science, [online] 189, pp.156–166. doi:https://doi.org/10.1016/j.procs.2021.05.080."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
