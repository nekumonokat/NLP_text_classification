{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d423c27a",
   "metadata": {},
   "source": [
    "<h2><center>NLP Text Classification</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3d0067",
   "metadata": {},
   "source": [
    "## I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1101e842",
   "metadata": {},
   "source": [
    "### 1.1 Domain-specific area\n",
    "This project provides an analysis of textual data on Twitter to accurately detect and classify threatening or harmful content using sentiment analysis techniques. This would provide the cybersecurity industry a tool that takes in a corpus of text for training to develop a strong detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72286743",
   "metadata": {},
   "source": [
    "### 1.2 Objectives\n",
    "Due to popular algorithms being centered around the detection of cyberbullying on social media (Cynthia Van Hee et al., 2018), it is important for this project to widen the scope of detection. While the general detection algorithms focus mainly on terrorism and cyberbullying, it is a known fact that cybersecurity encompasses more than those 2 focuses. (Khairy, Mahmoud and Abd-El-Hafeez, 2021) While full security and safety of users cannot be ensured, making these adjustments would contribute valuable insights for future development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd0cf1",
   "metadata": {},
   "source": [
    "### 1.3 Dataset\n",
    "To begin this project, an extensive amount of textual data corpora is required. After researching large datasets of Tweets, Sentiment140 Kaggle was proven to be the best for this project. With 1.6 million tweets extracted using the Twitter API, the authors have categorised each tweet to have either a positive, neutral or negative sentiment, which is beneficial for the algorithm in categorising harmful texts.\n",
    "\n",
    "The dataset consists of the target (defined as the sentiment of the text), the tweet IDs, date, flags (possible queries, which would be removed in the initialisation phase of extracting the data), the username, and the text of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f381e0e6",
   "metadata": {},
   "source": [
    "### 1.4 Evaluation methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7d3b22",
   "metadata": {},
   "source": [
    "## II. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e1df4",
   "metadata": {},
   "source": [
    "### 2.1 Pre-processing\n",
    "(writeup not needed)\n",
    "<br>Convert/store the dataset locally and preprocess the data. Describe the text representation\n",
    "(e.g., bag of words, word embedding, etc.) and any pre-processing steps you have applied\n",
    "and why they were needed (e.g. tokenization, lemmatization). Describe the vocabulary and\n",
    "file type/format, e.g. CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38466cb4",
   "metadata": {},
   "source": [
    "#### Acquiring dataset\n",
    "The dataset on the collection of Tweets were acquired from Kaggle by downloading the CSV file. The author of this dataset is Μαριος Μιχαηλιδης KazAnova. The code for importing the dataset is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641807df",
   "metadata": {},
   "source": [
    "#### 2.1.1 Importing libraries\n",
    "- <b>pandas library</b> was imported to process and handle datasets in Python. It is used to help write and read from CSV files while handling real-world messy data and processing them into a proper format\n",
    "\n",
    "- <b>numpy library</b> was imported to handle calculations and use numpy arrays for statistical calculations\n",
    "\n",
    "- <b>matplotlib library</b> was imported to plot the data and represent it graphically\n",
    "\n",
    "- <b>os library</b> was imported to have a way of using the operating system dependent functionalities, more specifically to save the dataset as a CSV\n",
    "\n",
    "- <b>word_tokenize library</b> divides strings into lists of substrings, this aids in regular expression\n",
    "\n",
    "- <b>stopwords library</b> was imported to have a library of the most common words in data to aid in stopwords removal\n",
    "\n",
    "- <b>string library</b> contains all ASCII characters considered as whitespace to aid in stopwords removal\n",
    "\n",
    "- <b>re library</b> specifies a set of strings that matches it and checks if a string matches a given regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26cf64bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sbgka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# dataframes\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from io import StringIO\n",
    "\n",
    "# text processing and analysis\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aacc76",
   "metadata": {},
   "source": [
    "#### 2.1.2 Creating helper functions\n",
    "- <b>displaysetsH</b> takes in a list of datasets and an optional number of rows to display the head of each dataset\n",
    "\n",
    "- <b>displaysetsT</b> takes in a list of datasets and an optional number of rows to display the tail of each dataset\n",
    "\n",
    "- <b>resetidx</b> takes in a list of datasets to reset the indexes of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9075d75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displaysetsH(datasets, amt = 5):\n",
    "    for dataset in datasets:\n",
    "        display(dataset.head(amt))\n",
    "        \n",
    "def displaysetsT(datasets, amt = 5):\n",
    "    for dataset in datasets:\n",
    "        display(dataset.head(amt))\n",
    "        \n",
    "def resetidx(datasets):\n",
    "    for dataset in datasets:\n",
    "        dataset.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39c4f8",
   "metadata": {},
   "source": [
    "#### 2.1.3 Importing dataset\n",
    "Due to the dataset being too large for analysis, it will analyse the first 2000 random textual data corpora, 1000 negative and positive sentiment to ensure valid analysis. We will look at the first entry to check if there are headers.\n",
    "\n",
    "Since there are no headers, we will add the condition 'header = None' when creating the dataframe for observation. The headers will also be created to aid in future analysis. The columns 'tweet_id', 'date', 'flag' and 'user' will then be removed as the project focus is on the sentiment analysis. This modified dataset will then be stored as a new CSV file.\n",
    "\n",
    "(<i>the headers are modified based on looking at the contents from Kaggle</i>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c6bb523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
       "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
       "\n",
       "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0  is upset that he can't update his Facebook by ...                                                                   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/sentiment140_dataset.csv', nrows = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b98626b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>4</td>\n",
       "      <td>I have this strange desire to go to confession...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>4</td>\n",
       "      <td>@i_reporter answer sent in dm. try it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>4</td>\n",
       "      <td>@brooklynunion cuz ur 3pm is my 9am and Id be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>4</td>\n",
       "      <td>@littrellfans Its all good. Just figured you w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>4</td>\n",
       "      <td>@nicolerichie Yea I remember it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                              tweet\n",
       "0             0  is upset that he can't update his Facebook by ...\n",
       "1             0  @Kenichan I dived many times for the ball. Man...\n",
       "2             0    my whole body feels itchy and like its on fire \n",
       "3             0  @nationwideclass no, it's not behaving at all....\n",
       "4             0                      @Kwesidei not the whole crew \n",
       "...         ...                                                ...\n",
       "1995          4  I have this strange desire to go to confession...\n",
       "1996          4             @i_reporter answer sent in dm. try it \n",
       "1997          4  @brooklynunion cuz ur 3pm is my 9am and Id be ...\n",
       "1998          4  @littrellfans Its all good. Just figured you w...\n",
       "1999          4                   @nicolerichie Yea I remember it \n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'datasets/sentiment140_dataset.csv'\n",
    "headers = ['sentiment', 'tweet_id', 'date', 'flag', 'user', 'tweet']\n",
    "\n",
    "random_entries = []\n",
    "positives = 1000  # positive sentiment counters\n",
    "negatives = 1000  # negative sentiment counters\n",
    "\n",
    "with open(filename, 'r', encoding='latin-1') as file:\n",
    "    # Skip the first line if it contains headers\n",
    "    headers = next(file, None)\n",
    "    \n",
    "    for line in file:\n",
    "        sentiment = int(line.split(\",\")[0].strip(\"\\\"\"))  # Extract and strip the sentiment value\n",
    "        \n",
    "        if (sentiment == 0 and negatives > 0) or (sentiment == 2 and neutrals > 0) or (sentiment == 4 and positives > 0):\n",
    "            random_entries.append(line)\n",
    "            \n",
    "            if sentiment == 0:\n",
    "                negatives -= 1\n",
    "            elif sentiment == 2:\n",
    "                neutrals -= 1\n",
    "            elif sentiment == 4:\n",
    "                positives -= 1\n",
    "                \n",
    "            if negatives == 0 and neutrals == 0 and positives == 0:\n",
    "                break\n",
    "\n",
    "tweets_df = pd.read_csv(StringIO(\"\".join(random_entries)), header = None)\n",
    "headers = ['sentiment', 'tweet_id', 'date', 'flag', 'user', 'tweet']\n",
    "tweets_df.columns = headers\n",
    "columns_to_drop = ['tweet_id', 'date', 'flag', 'user']\n",
    "tweets_df.drop(columns = columns_to_drop, inplace = True)\n",
    "tweets_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b439f3",
   "metadata": {},
   "source": [
    "The tweets_df will now be saved into a new dataset for easier accessibility. To ensure that no duplicates are saved, a simple path checking will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e132cf8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully.\n"
     ]
    }
   ],
   "source": [
    "file_path = 'datasets/selected_sentiment140_dataset.csv'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    tweets_df.to_csv(file_path, index = False)\n",
    "    print('File saved successfully.')\n",
    "else:\n",
    "    print('File already exists.')\n",
    "   \n",
    "tweets_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e6e51f",
   "metadata": {},
   "source": [
    "#### 2.1.5 Checking for duplicates and null entries\n",
    "To ensure that the analysis is beneficial, all 2000 entries that were chosen should be unique. To do so, a 'duplicated' column will be added to the a temporary copy of the dataset which is the output of df.duplicated() and we will print only columns where the 'duplicated' column is True.\n",
    "\n",
    "Based on the output, it is seen that there are no duplicated Tweets.\n",
    "\n",
    "After ensuring that all entries are unique, a check that all entries do not have NULL entries will be done. This is because sentiment analysis cannot be done on empty texts.\n",
    "\n",
    "Based on the output, it is seen that there are no NULL entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b88b7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet</th>\n",
       "      <th>duplicated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sentiment, tweet, duplicated]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dupe_checker = tweets_df.copy()\n",
    "duplicates = dupe_checker.duplicated()\n",
    "dupe_checker['duplicated'] = duplicates\n",
    "duplicated = dupe_checker[dupe_checker['duplicated'] == True]\n",
    "duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50595fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    0\n",
       "tweet        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_checker = tweets_df.copy()\n",
    "null_checker.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44983f22",
   "metadata": {},
   "source": [
    "#### 2.1.6 Basic statistics\n",
    "To ensure that the data is good for analysis, the amount of positive, neutral and negative sentiments should be as balanced as possible (sentiment values: 0 = negative, 2 = neutral, 4 = positive)\n",
    "\n",
    "Based on the output, it is seen that the dataset has been skewed. However, due to the focus on the project being the detection of threatful comments, the skewed focus on negative sentiments should be sufficient enough for detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca7eb29",
   "metadata": {},
   "source": [
    "** Second option: Based on the output, it is seen that the dataset has been skewed. Due to this, a new collation of the first and last 1000 entries will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f86dd81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive sentiments: sentiment    1000\n",
      "tweet        1000\n",
      "dtype: int64\n",
      "Number of negative sentiments: sentiment    1000\n",
      "tweet        1000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sentiment_checker = tweets_df.copy()\n",
    "print('Number of positive sentiments:', sentiment_checker[sentiment_checker['sentiment'] == 4].count())\n",
    "print('Number of negative sentiments:', sentiment_checker[sentiment_checker['sentiment'] == 0].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58efb49a",
   "metadata": {},
   "source": [
    "#### 2.1.7 Basic text processing\n",
    "To begin the process of analysing the text, we would require conducting basic text processing methods.\n",
    "\n",
    "TO DO LIST:\n",
    "- Describe the text representation (e.g., bag of words, word embedding, etc.) **[not done]**\n",
    "- Describe the vocabulary and file type/format, e.g. CSV file. [**not done**]\n",
    "- any pre-processing steps you have applied and why they were needed (e.g. tokenization [**done**], lemmatization [**did regex**])."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac1183",
   "metadata": {},
   "source": [
    "- <b>Removing stop words</b>: In human language, it is very common for stop words to be present. These words, including **determiners** (eg: the, a, this, my), **conjunctions** (eg: and, or, nor, but, whereas) and **prepositions** (eg: against, along, at, before), are used to connect thoughts and speech to form grammatically accurate sentences or structural cohesion. While important during communication amongst one another, they do not carry importance or sentiments that would be valuable to this project, thus introducing noise. The removal would help to streamline the process to focus on words that would contribute more meaning to the sentiment of Tweets.\n",
    "\n",
    "* Tokenisation will be done in lowercase as all stopwords are in lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading stopword corpus\n",
    "nltk.download('stopwords')\n",
    "# Get stopword list\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd5d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking removal works on a test text\n",
    "test_tweet = ['This is a test that Stopword removal works.']\n",
    "\n",
    "def filter_text(tweets):\n",
    "    for tweet in tweets:\n",
    "        tokens = tweet.lower().split()\n",
    "        # Removing each token if part of stop_words\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        print('filtered_tokens', filtered_tokens)\n",
    "        \n",
    "filter_text(test_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before improvement of stopwords\n",
    "tweets = tweets_df['tweet'].tolist()\n",
    "filter_text(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0871932",
   "metadata": {},
   "source": [
    "The removal of stopwords has reduced the texts. However after analysing the filtered words, it is seen that symbols are still considered, tokens. As such, the **removal of symbols** would be added to stop_words. Due to the tokenization making ending words with symbols (eg: 'goodbye!') a single token, the removal will only be on tokens with no characters (eg: '!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757f752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "symbols = set(string.punctuation)\n",
    "\n",
    "# Checking removal works on a test text\n",
    "test_tweet2 = ['modified test that StopwOrd removal works. It will remove @testtry']\n",
    "\n",
    "def filter_text2(tweets):\n",
    "    for tweet in tweets:\n",
    "        tokens = tweet.lower().split()\n",
    "        \n",
    "        filtered_tokens = [token for token in tokens if not (\n",
    "            # Removing each token if part of symbols\n",
    "            all(char in symbols for char in token)\n",
    "            # Removing each token if part of stop_words\n",
    "            or token in stop_words\n",
    "        )]\n",
    "        \n",
    "        print('filtered_tokens', filtered_tokens)\n",
    "        \n",
    "filter_text2(test_tweet2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c72b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After improvement of stopwords\n",
    "tweets = tweets_df['tweet'].tolist()\n",
    "filter_text2(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3702e",
   "metadata": {},
   "source": [
    "- <b>Regular expressions (Regex)</b>: Regex are known to have highly optimised algorithms. By choosing the patterns that we want to recognise, it allows us to handle variations better. As such, we will use this to get rid of **links and tagging**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e82a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "symbols = set(string.punctuation)\n",
    "\n",
    "# Checking removal works on a test text\n",
    "test_tweet3 = ['modified test that stopword removal works. It will remove @testtry', \n",
    "               'http not removed but https://www.youtube.com/ and http://www.youtube.com/ removed']\n",
    "\n",
    "def filter_text3(tweets):\n",
    "    for tweet in tweets:\n",
    "        # Tokenizing URLs using regular expression\n",
    "        tweet = re.sub(r'@\\S+|(?:http|https)://\\S+', '', tweet)\n",
    "        tokens = tweet.lower().split()\n",
    "        \n",
    "        filtered_tokens = [token for token in tokens if not (\n",
    "            # Removing each token if part of symbols\n",
    "            all(char in symbols for char in token)\n",
    "            # Removing each token if part of stop_words\n",
    "            or token in stop_words\n",
    "        )]\n",
    "        \n",
    "        print('filtered_tokens', filtered_tokens)\n",
    "        \n",
    "filter_text3(test_tweet3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c27f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After improvement of stopwords\n",
    "tweets = tweets_df['tweet'].tolist()\n",
    "filter_text3(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db436f2",
   "metadata": {},
   "source": [
    "Users are known to add **repeated trailing symbols** or alphabets as a way to express their emotions. We will make use of Regex to show it as a single occurance. When considering the reduction of alphabets, we have to take into consideration whether the word has been dragged, or if the word does have repeated characters. (eg: kangaroo)\n",
    "\n",
    "As the common dictionary does not seem to have any words that have 3 consecutive repeated letters, we would tokenise the words to have repeated characters a maximum of 2 times. While this is not a full-proof way of resolving repeated trailing characters, this is the safest way to ensure better classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "symbols = set(string.punctuation)\n",
    "\n",
    "# Checking removal works on a test text\n",
    "test_tweet4 = ['modified test that stopword removal works. It will remove @testtry', \n",
    "               'http not removed but https://www.youtube.com/ and http://www.youtube.com/ removed',\n",
    "               'The cleaning will show this!!!!!!!!?!!!?!!!!!! with 1 exclaimation',\n",
    "               'kaaaaaaangarooooooooooooooooooooooooooo will be cleaned up']\n",
    "\n",
    "def filter_text4(tweets):\n",
    "    for tweet in tweets:\n",
    "        # Tokenizing URLs using Regex\n",
    "        tweet = re.sub(r'@\\S+|(?:http|https)://\\S+', '', tweet)\n",
    "        # Tokenizing repeated symbols using Regex\n",
    "        tweet = re.sub(r'([!@#$%^&*.]){2,}', r'\\1', tweet)\n",
    "        # Tokenizing repeated characters\n",
    "        tweet = re.sub(r'([A-Za-z])\\1+', r'\\1\\1', tweet)\n",
    "        tokens = tweet.lower().split()\n",
    "        \n",
    "        filtered_tokens = [token for token in tokens if not (\n",
    "            # Removing each token if part of symbols\n",
    "            all(char in symbols for char in token)\n",
    "            # Removing each token if part of stop_words\n",
    "            or token in stop_words\n",
    "        )]\n",
    "        \n",
    "        print('filtered_tokens', filtered_tokens)\n",
    "        \n",
    "filter_text4(test_tweet4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c04e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After improvement of stopwords\n",
    "tweets = tweets_df['tweet'].tolist()\n",
    "filter_text4(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2fd548",
   "metadata": {},
   "source": [
    "### 2.2 Baseline performance\n",
    "(writeup not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06ca830",
   "metadata": {},
   "source": [
    "### 2.3 Classification approach\n",
    "(writeup not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d0d9cc",
   "metadata": {},
   "source": [
    "### 2.4 Coding style\n",
    "(writeup not needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c187d",
   "metadata": {},
   "source": [
    "## III. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c2562",
   "metadata": {},
   "source": [
    "### 3.1 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99e3055",
   "metadata": {},
   "source": [
    "### 3.2 Summary and conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950434fd",
   "metadata": {},
   "source": [
    "## Temporary reference list\n",
    "* to use citation generator\n",
    "\n",
    "- Cynthia Van Hee, Jacobs, G., Emmery, C., Desmet, B., Lefever, E., Verhoeven, B., Guy De Pauw, Daelemans, W. and Hoste, V. (2018). Automatic detection of cyberbullying in social media text. PLOS ONE, [online] 13(10), p.e0203794. doi:https://doi.org/10.1371/journal.pone.0203794.\n",
    "- Khairy, M., Mahmoud, T.M. and Abd-El-Hafeez, T. (2021). Automatic Detection of Cyberbullying and Abusive Language in Arabic Content on Social Networks: A Survey. Procedia Computer Science, [online] 189, pp.156–166. doi:https://doi.org/10.1016/j.procs.2021.05.080."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
